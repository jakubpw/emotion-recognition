{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Selected angles and edges for emotion analysis with face mesh point map from media pipe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "selected_face_mesh_points_dict = {\n",
    "    0: 61,\n",
    "    1: 292,\n",
    "    2: 0,\n",
    "    3: 17,\n",
    "    4:\t50,\n",
    "    5:\t280,\n",
    "    6:\t48,\n",
    "    7:\t4,\n",
    "    8:\t278,\n",
    "    9:  206,\n",
    "    10:\t426,\n",
    "    11:\t133,\n",
    "    12:\t130,\n",
    "    13:\t159,\n",
    "    14:\t145,\n",
    "    15:\t362,\n",
    "    16:\t359,\n",
    "    17:\t386,\n",
    "    18:\t374,\n",
    "    19:\t122,\n",
    "    20:\t351,\n",
    "    21:\t46,\n",
    "    22:\t105,\n",
    "    23:\t107,\n",
    "    24:\t276,\n",
    "    25:\t334,\n",
    "    26:\t336,\n",
    "}\n",
    "\n",
    "angles_points = [\n",
    " [2, 0,3],\n",
    " [0, 2,1],\n",
    " [6, 7, 8],\n",
    " [9, 7, 10],\n",
    " [0, 7, 1],\n",
    " [1, 5, 8],\n",
    " [0, 4, 6],\n",
    " [1, 10, 8],\n",
    " [0, 9, 6],\n",
    " [13, 12, 14],\n",
    " [12, 13, 11],\n",
    " [17, 15, 18],\n",
    " [15, 17, 16],\n",
    " [21, 22, 23],\n",
    " [26, 25, 24],\n",
    " [6, 19, 23],\n",
    " [8, 20, 26],\n",
    "]\n",
    "\n",
    "selected_face_mesh_edges = []\n",
    "selected_face_mesh_angles = []\n",
    "for x in angles_points:\n",
    "    selected_face_mesh_edges.append([selected_face_mesh_points_dict[x[0]], selected_face_mesh_points_dict[x[1]]])\n",
    "    selected_face_mesh_edges.append([selected_face_mesh_points_dict[x[1]], selected_face_mesh_points_dict[x[2]]])\n",
    "    selected_face_mesh_angles.append([selected_face_mesh_points_dict[y] for y in x])\n",
    "\n",
    "def dot_product_angle(v1,v2):\n",
    "    vector_dot_product = np.dot(v1,v2)\n",
    "    arccos = np.arccos(vector_dot_product / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "    res = np.degrees(arccos)\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_angle(x1, x2, x3):\n",
    "    v1 = [x1.x - x2.x, x1.y - x2.y, x1.z - x2.z]\n",
    "    v2 = [x3.x - x2.x, x3.y - x2.y, x3.z - x2.z]\n",
    "    return dot_product_angle(v1, v2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T13:27:47.967060Z",
     "start_time": "2023-11-25T13:27:47.652145Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Detecting selected angles on video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "def extract_landmarks(mediapipe_results):\n",
    "    if mediapipe_results.multi_face_landmarks:\n",
    "        face_landmark = mediapipe_results.multi_face_landmarks[0].landmark\n",
    "        return True, [get_angle(face_landmark[face_angel[0]], face_landmark[face_angel[1]], face_landmark[face_angel[2]])\n",
    "                      for face_angel in selected_face_mesh_angles]\n",
    "    return False, []\n",
    "\n",
    "\n",
    "def detect_landmarks(video_path, measurements_per_second = 5):\n",
    "    capture = cv.VideoCapture(video_path)\n",
    "\n",
    "    fps = capture.get(cv.CAP_PROP_FPS)\n",
    "    frame_step = math.ceil(fps / measurements_per_second)\n",
    "    frame_count = int(capture.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "    frame_ordinal = measurements_per_second\n",
    "\n",
    "    video_landmarks: List[Optional[List[Tuple[float, float, float]]]] = []\n",
    "\n",
    "    with mp.solutions.face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=True) as face_mesh:\n",
    "        while True:\n",
    "            if video_path:\n",
    "                capture.set(cv.CAP_PROP_POS_FRAMES, frame_ordinal)\n",
    "            frame_exists, frame = capture.read()\n",
    "\n",
    "            if not frame_exists or frame_ordinal > frame_count:\n",
    "                capture.release()\n",
    "                cv.destroyAllWindows()\n",
    "                return video_landmarks\n",
    "\n",
    "            frame = cv.flip(frame, 1)\n",
    "            rgb_frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "            face_detected, frame_landmarks = extract_landmarks(face_mesh.process(rgb_frame))\n",
    "            video_landmarks.append(frame_landmarks if face_detected else None)\n",
    "\n",
    "            frame_ordinal += frame_step"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyzing RAVDESS dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "RAVDESS_DATASET_PATH = \"datasets/video/RAVDESS/\"\n",
    "VIDEO_FOLDERS = os.listdir(RAVDESS_DATASET_PATH)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "num = 0\n",
    "with mp.solutions.face_mesh.FaceMesh(\n",
    "        static_image_mode=False,\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as face_mesh:\n",
    "    for video_folder in VIDEO_FOLDERS:\n",
    "        VIDEO_FILES = os.listdir(RAVDESS_DATASET_PATH + video_folder)\n",
    "        print(video_folder)\n",
    "        for file in VIDEO_FILES:\n",
    "            if file[1] == '2' and file[7] != '7':\n",
    "                video_frames = detect_landmarks(\"actors/\" + video_folder + \"/\" + file)\n",
    "                for frame_no, video_frame in enumerate(video_frames):\n",
    "                        if video_frame is not None:\n",
    "                            df.at[num, \"frame_num\"] = frame_no\n",
    "                            df.at[num, \"video\"] = file\n",
    "                            for i, angel in enumerate(selected_face_mesh_angles):\n",
    "                                df.at[num, angel.__str__()] = video_frame[i]\n",
    "                            num = num + 1\n",
    "                        \n",
    "ravdess_em_dict = {\n",
    "    1: 5,\n",
    "    2: 5,\n",
    "    3: 0,\n",
    "    4: 1,\n",
    "    5: 3,\n",
    "    6: 2,\n",
    "    8: 4\n",
    "}\n",
    "df['emotion'] = df.apply(lambda x: ravdess_em_dict[int(x['video'][7])], axis=1)\n",
    "\n",
    "df.to_csv('ravdess.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyzing RAF dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "RAF_DATASET_PATH = \"datasets/image/RAF/Image/original/\"\n",
    "\n",
    "IMAGE_FILES = os.listdir(RAF_DATASET_PATH)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(IMAGE_FILES)\n",
    "for angle in selected_face_mesh_angles:\n",
    "    df[angle.__str__()] = None\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True,\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5) as face_mesh:\n",
    "    for idx, file in enumerate(IMAGE_FILES):\n",
    "        image = cv2.imread(RAF_DATASET_PATH + file)\n",
    "        results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        if not results.multi_face_landmarks:\n",
    "            continue\n",
    "        for face_no, face_landmarks in enumerate(results.multi_face_landmarks):\n",
    "            for angel in selected_face_mesh_angles:\n",
    "                df.at[idx, angel.__str__()] = get_angle(face_landmarks.landmark[angel[0]], face_landmarks.landmark[angel[1]], face_landmarks.landmark[angel[2]])\n",
    "\n",
    "df2 = pd.read_csv(\"datasets/image/RAF/EmoLabel/list_patition_label.txt\", sep=\" \", header=None)\n",
    "df = df.merge(df2)\n",
    "df = df[df[1] != 3]\n",
    "raf_em_dict = {\n",
    "    1: 4,\n",
    "    2: 2,\n",
    "    4: 0,\n",
    "    5: 1,\n",
    "    6: 3,\n",
    "    7: 5\n",
    "}\n",
    "df['emotion'] = df.apply(lambda x: raf_em_dict[x[1]], axis=1)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df = df.drop(1, axis=1)\n",
    "df.to_csv('raf.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T01:20:28.432802Z",
     "start_time": "2023-11-26T01:16:28.682053Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analyzing FER dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-25T21:53:12.368935Z",
     "start_time": "2023-11-25T21:52:04.200200Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "FER_DATASET_PATH = \"datasets/image/FER/test/\"\n",
    "IMAGE_DIRS = os.listdir(FER_DATASET_PATH)\n",
    "IMAGE_FILES = []\n",
    "for element in IMAGE_DIRS:\n",
    "    if not element.startswith(\".\"):\n",
    "        IMAGE_FILES = IMAGE_FILES + [element  + \"/\" + x for x in os.listdir(FER_DATASET_PATH + element)]\n",
    "\n",
    "df = pd.DataFrame(IMAGE_FILES)\n",
    "for angle in selected_face_mesh_angles:\n",
    "    df[angle.__str__()] = None\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True,\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5) as face_mesh:\n",
    "        for idx, file in enumerate(IMAGE_FILES):\n",
    "            image = cv2.imread(FER_DATASET_PATH + file)\n",
    "            # Convert the BGR image to RGB before processing.\n",
    "            results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            # Print and draw face mesh landmarks on the image.\n",
    "            if not results.multi_face_landmarks:\n",
    "                continue\n",
    "            for face_no, face_landmarks in enumerate(results.multi_face_landmarks):\n",
    "                for angel in selected_face_mesh_angles:\n",
    "                    df.at[idx, angel.__str__()] = get_angle(face_landmarks.landmark[angel[0]], face_landmarks.landmark[angel[1]], face_landmarks.landmark[angel[2]])\n",
    "fer_em_dict = {\n",
    "    \"hap\": 0,\n",
    "    \"sad\": 1,\n",
    "    \"fea\": 2,\n",
    "    \"ang\": 3,\n",
    "    \"sur\": 4,\n",
    "    \"neu\": 5\n",
    "}\n",
    "df['emotion'] = df.apply(lambda x: fer_em_dict[x[0][:3]], axis=1)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df.to_csv('fer_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load analyzed data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raf_data = pd.read_csv('raf.csv', index_col=0)\n",
    "raf_train = raf_data[raf_data['0'].str.startswith('tr')]\n",
    "raf_test = raf_data[raf_data['0'].str.startswith('te')]\n",
    "X_raf_train = raf_train.drop(['0', 'emotion'], axis=1)\n",
    "y_raf_train = raf_train['emotion']\n",
    "X_raf_test = raf_test.drop(['0', 'emotion'], axis=1)\n",
    "y_raf_test = raf_test['emotion']\n",
    "\n",
    "fer_train = pd.read_csv('analyzed_datasets/fer_train.csv', index_col=0)\n",
    "fer_test = pd.read_csv('analyzed_datasets/fer_test.csv', index_col=0)\n",
    "X_fer_train = fer_train.drop(['0', 'emotion'], axis=1)\n",
    "y_fer_train = fer_train['emotion']\n",
    "X_fer_test = fer_test.drop(['0', 'emotion'], axis=1)\n",
    "y_fer_test = fer_test['emotion']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T16:50:39.543221Z",
     "start_time": "2023-12-03T16:50:39.246219Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train RandomForestClassifier and LogisticRegression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "models_raf = [RandomForestClassifier(random_state=0, max_depth=10), LogisticRegression(max_iter=5000, random_state=0)]\n",
    "models_fer = [RandomForestClassifier(random_state=0, max_depth=10), LogisticRegression(max_iter=5000, random_state=0)]\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "X_raf_train_oversampled, y_raf_train_oversampled = oversample.fit_resample(X_raf_train, y_raf_train)\n",
    "X_fer_train_oversampled, y_fer_train_oversampled = oversample.fit_resample(X_fer_train, y_fer_train)\n",
    "target_names = ['Happiness', 'Sadness', 'Fear', 'Anger', 'Surprise', 'Neutral']\n",
    "for model in models_raf:\n",
    "    model.fit(X_raf_train_oversampled, y_raf_train_oversampled)\n",
    "    y_pred = model.predict(X_raf_test)\n",
    "    print(model)\n",
    "    print(classification_report(y_raf_test, y_pred, target_names=target_names))\n",
    "for model in models_fer:\n",
    "    model.fit(X_fer_train_oversampled, y_fer_train_oversampled)\n",
    "    y_pred = model.predict(X_fer_test)\n",
    "    print(model)\n",
    "    print(classification_report(y_fer_test, y_pred, target_names=target_names))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test models on datasets other than those on which they were trained"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model in models_raf:\n",
    "    y_pred = model.predict(X_fer_test)\n",
    "    print(model)\n",
    "    print(classification_report(y_fer_test, y_pred, target_names=target_names))\n",
    "for model in models_fer:\n",
    "    y_pred = model.predict(X_raf_test)\n",
    "    print(model)\n",
    "    print(classification_report(y_raf_test, y_pred, target_names=target_names))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate random forest on RAVDESS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "ravdess_data = pd.read_csv('analyzed_datasets/ravdess.csv', index_col=0)\n",
    "y_pred = []\n",
    "for i in range(5):\n",
    "    racdess_one_frame = ravdess_data[ravdess_data['frame_num'] == 7.0 + i]\n",
    "    X_ravdess_one_frame = racdess_one_frame.drop(['frame_num', 'emotion', 'video'], axis=1)\n",
    "    y_pred.append(models_raf[0].predict(X_ravdess_one_frame))\n",
    "racdess_one_frame = ravdess_data[ravdess_data['frame_num'] == 5.0]\n",
    "y_ravdess_one_frame = racdess_one_frame['emotion']\n",
    "res = stats.mode(np.array(y_pred))[0]\n",
    "print(classification_report(y_ravdess_one_frame, res, target_names=target_names))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load analyzed RAVDESS dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "ravdess_data = pd.read_csv('analyzed_datasets/ravdess.csv', index_col=0)\n",
    "X_ravdess_data = ravdess_data.drop(['frame_num', 'emotion', 'video'], axis=1)\n",
    "y_ravdess_data = ravdess_data['emotion']\n",
    "Y = pd.get_dummies(ravdess_data['emotion']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_ravdess_data,Y, test_size = 0.10, random_state = 42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T09:21:53.709195Z",
     "start_time": "2023-12-03T09:21:43.427079Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define LSTM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "from keras.src.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units = 200, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(units = 200, return_sequences = True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(units = 50))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'Precision', 'Recall'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T20:10:03.492175Z",
     "start_time": "2023-12-03T20:10:00.530305Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train LSTM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "accr = model.evaluate(X_test,Y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LSTM classification report"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "Y_test_int = np.argmax(Y_test, axis=1)\n",
    "print(classification_report(Y_test_int, y_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convert LSTM to tensorflowjs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "tfjs.converters.save_keras_model(model, \"model4\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
